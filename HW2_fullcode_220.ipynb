{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2bw3oqYwtuH"
   },
   "source": [
    "# <center>  SI 630: Homework 2: Word2Vec\n",
    "<center>  Codalab Usename: Wkailin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cHO_b6RwtuJ",
    "outputId": "bcd3801b-8fd3-4748-8227-d3b02ba4ee88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b155ca4ee10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install torch torchvision\n",
    "# %pip install gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Helpful for computing cosine similarity--Note that this is NOT a similarity!\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Handy command-line argument parsing\n",
    "import argparse\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# We'll use this to save our models\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#\n",
    "# IMPORTANT NOTE: Always set your random seeds when dealing with stochastic\n",
    "# algorithms as it lets your bugs be reproducible and (more importantly) it lets\n",
    "# your results be reproducible by others.\n",
    "#\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjdrMCnzwtuL"
   },
   "source": [
    "#### Create a class to hold the data\n",
    "\n",
    "Before we get to training word2vec, we'll need to process the corpus into some representation. The `Corpus` class will handle much of the functionality for corpus reading and keeping track of which word types belong to which ids. The `Corpus` class will also handle the crucial functionality of generating negative samples for training (i.e., randomly-sampled words that were not in the target word's context).\n",
    "\n",
    "Some parts of this class can be completed after you've gotten word2vec up and running, so see the notes below and the details in the homework PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CcTK_8E5wtuN"
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "        # These state variables become populated with function calls\n",
    "        #\n",
    "        # 1. load_data()\n",
    "        # 2. generate_negative_sampling_table()\n",
    "        # See those functions for how the various values get filled in\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        # How many times each word occurs in our data after filtering\n",
    "        self.word_counts = Counter()\n",
    "\n",
    "        # A utility data structure that lets us quickly sample \"negative\"\n",
    "        # instances in a context. This table contains unique-ids\n",
    "        self.negative_sampling_table = []\n",
    "        \n",
    "        # The dataset we'll use for training, as a sequence of unqiue word\n",
    "        # ids. This is the sequence across all documents after tokens have been\n",
    "        # randomly subsampled by the word2vec preprocessing step\n",
    "        self.full_token_sequence_as_ids = None\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        '''\n",
    "        Tokenize the document and returns a list of the tokens\n",
    "        '''\n",
    "        return self.tokenizer.tokenize(text)        \n",
    "\n",
    "## Modify function load data in the Corpus class to read in the text data and fill in \n",
    "## the id to word, word to id, and full token sequence as ids fields\n",
    "    def load_data(self, file_name, min_token_freq):\n",
    "        '''\n",
    "        Reads the data from the specified file as long long sequence of text\n",
    "        (ignoring line breaks) and populates the data structures of this\n",
    "        word2vec object.\n",
    "        '''\n",
    "\n",
    "        # Step 1: Read in the file and create a long sequence of tokens for\n",
    "        # all tokens in the file\n",
    "        with open(file_name,'r') as f:\n",
    "            text = f.readlines()\n",
    "        # convert to lower case\n",
    "        text = [sen.lower() for sen in text]\n",
    "        all_tokens = self.tokenize(' '.join(text))\n",
    "        print('Reading data and tokenizing')\n",
    "        \n",
    "        # Step 2: Count how many tokens we have of each type\n",
    "        count_tokens = Counter(all_tokens)\n",
    "        print('Counting token frequencies')\n",
    "\n",
    "        # Step 3: Replace all tokens below the specified frequency with an <UNK>\n",
    "        # token. \n",
    "        count_tokens_unk = {key:value for key,value in count_tokens.items() \n",
    "                            if value>=min_token_freq\n",
    "                           }\n",
    "        count_tokens_unk['<UNK>'] = sum(\n",
    "            np.array(list(count_tokens.values()))\n",
    "            [np.array(list(count_tokens.values()))<min_token_freq])\n",
    "        # NOTE: You can do this step later if needed\n",
    "        print(\"Performing minimum thresholding\")\n",
    "        \n",
    "        # Step 4: update self.word_counts to be the number of times each word\n",
    "        # occurs (including <UNK>)\n",
    "        self.word_counts = count_tokens_unk # counter{tokens:count}\n",
    "        # Step 5: Create the mappings from word to unique integer ID and the\n",
    "        # reverse mapping.\n",
    "        # counter{tokens: token_ids}\n",
    "        self.word_to_index = dict(zip(list(self.word_counts.keys()),\n",
    "                                      np.arange(len(self.word_counts))\n",
    "                                     )\n",
    "                                 )\n",
    "        self.index_to_word = dict(zip(np.arange(len(self.word_counts)),\n",
    "                                 list(self.word_counts.keys())\n",
    "                                     )\n",
    "                                 )\n",
    "        # Step 6: Compute the probability of keeping any particular *token* of a\n",
    "        # word in the training sequence, which we'll use to subsample. This subsampling\n",
    "        # avoids having the training data be filled with many overly common words\n",
    "        # as positive examples in the context\n",
    "        n = sum(self.word_counts.values())\n",
    "        word_prob = np.array(list(self.word_counts.values()))/n #0.001\n",
    "        word_prob = (np.sqrt(word_prob/0.001)+1)*(0.001/word_prob)\n",
    "        word_to_sample_prob = dict(zip(self.index_to_word.keys(),word_prob))\n",
    "        # Step 7: process the list of tokens (after min-freq filtering) to fill\n",
    "        # self.full_token_sequence_as_ids: list of lists; unique ids\n",
    "        self.full_token_sequence_as_ids = [[]] * len(text)\n",
    "        for i in range(len(text)):\n",
    "            token_seq = []\n",
    "            for word in self.tokenize(text[i]):\n",
    "                if word not in self.word_to_index.keys():\n",
    "                    word = '<UNK>'\n",
    "                if word_to_sample_prob[self.word_to_index[word]] > np.random.uniform():\n",
    "                    token_seq.append(self.word_to_index[word])\n",
    "            self.full_token_sequence_as_ids[i] = token_seq\n",
    "                \n",
    "        # Helpful print statement to verify what you've loaded\n",
    "        print('Loaded all data from %s; saw %d tokens (%d unique)' \\\n",
    "              % (file_name, len(self.full_token_sequence_as_ids),\n",
    "                 len(self.word_to_index)))\n",
    "        \n",
    "    def generate_negative_sampling_table(self, exp_power=0.75, table_size=1e6):\n",
    "        '''\n",
    "        Generates a big list data structure that we can quickly randomly index into\n",
    "        in order to select a negative training example (i.e., a word that was\n",
    "        *not* present in the context). \n",
    "        '''       \n",
    "        # Step 1: Figure out how many instances of each word need to go into the\n",
    "        # negative sampling table. # HINT: np.power and np.fill might be useful here        \n",
    "        print(\"Generating sampling table\")\n",
    "        # num_w_neg: np.array, representing how many instances of each word need to go into the\n",
    "        # negative sampling table\n",
    "        num_w_neg = np.power(list(self.word_counts.values()), exp_power)\n",
    "        num_w_neg = np.round(np.cumsum(num_w_neg/sum(num_w_neg)*table_size))\n",
    "        num_w_neg = np.insert(np.diff(num_w_neg), 0, num_w_neg[0]).astype(int)      \n",
    "        # Step 2: Create the table to the correct size. You'll want this to be a\n",
    "        # numpy array of type int\n",
    "        # Step 3: Fill the table so that each word has a number of IDs\n",
    "        # proportionate to its probability of being sampled. \n",
    "        # table: int np.array to sample from\n",
    "        self.negative_sampling_table = np.repeat(np.array(list(self.word_to_index.values())) ,\n",
    "                          num_w_neg)\n",
    "        \n",
    "        # Example: if we have 3 words \"a\" \"b\" and \"c\" with probabilites 0.5,\n",
    "        # 0.33, 0.16 and a table size of 6 then our table would look like this\n",
    "        # (before converting the words to IDs):\n",
    "        #\n",
    "        # [ \"a\", \"a\", \"a\", \"b\", \"b\", \"c\" ]\n",
    "        #\n",
    "        pass\n",
    "\n",
    "\n",
    "    def generate_negative_samples(self, cur_context_word_id, num_samples):\n",
    "        '''\n",
    "        Randomly samples the specified number of negative samples from the lookup\n",
    "        table and returns this list of IDs as a numpy array. As a performance\n",
    "        improvement, avoid sampling a negative example that has the same ID as\n",
    "        the current positive context word.\n",
    "        '''\n",
    "        # Create a list and sample from the negative_sampling_table to\n",
    "        # grow the list to num_samples, avoiding adding a negative example that\n",
    "        # has the same ID as the current context_word\n",
    "        # excluding cur_context_word_id\n",
    "        n_neg_tab = len(self.negative_sampling_table)# get ids for neg samples\n",
    "        while True:\n",
    "            neg_id = random.sample(range(n_neg_tab), num_samples)\n",
    "            if not np.intersect1d(neg_id,cur_context_word_id):\n",
    "                break # no intersection\n",
    "        results = self.negative_sampling_table[neg_id]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16Q76G93wtuO"
   },
   "source": [
    "## Create the corpus\n",
    "\n",
    "Now that we have code to turn the text into training data, let's do so. We've provided several files for you to help:\n",
    "\n",
    "* `wiki-bios.DEBUG.txt` -- use this to debug your corpus reader\n",
    "* `wiki-bios.10k.txt` -- use this to debug/verify the whole word2vec works\n",
    "* `wiki-bios.med.txt` -- use this when everything works to generate your vectors for later parts\n",
    "* `wiki-bios.HUGE.txt.gz` -- _do not use this_ unless (1) everything works and (2) you really want to test/explore. This file is not needed at all to do your homework.\n",
    "\n",
    "We recommend startin to debug with the first file, as it is small and fast to load (quicker to find bugs). When debugging, we recommend setting the `min_token_freq` argument to 2 so that you can verify that part of the code is working but you still have enough word types left to test the rest.\n",
    "\n",
    "You'll use the remaining files later, where they're described.\n",
    "\n",
    "In the next cell, create your `Corpus`, read in the data, and generate the negative sampling table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from wiki-bios.DEBUG.txt; saw 100 tokens (712 unique)\n",
      "Generating sampling table\n"
     ]
    }
   ],
   "source": [
    "corpus_de = Corpus()\n",
    "corpus_de.load_data('wiki-bios.DEBUG.txt', 5)\n",
    "corpus_de.generate_negative_sampling_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "So2u5krlwtuO",
    "outputId": "7262f0cc-e1c8-418f-ee37-dce64c563604",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from wiki-bios.med.txt; saw 100000 tokens (96721 unique)\n",
      "Generating sampling table\n"
     ]
    }
   ],
   "source": [
    "corpus_med = Corpus()\n",
    "corpus_med.load_data('wiki-bios.med.txt', 5)\n",
    "corpus_med.generate_negative_sampling_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus_med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdRUsAyHwtuP"
   },
   "source": [
    "## Generate the training data\n",
    "\n",
    "Once we have the corpus ready, we need to generate our training dataset. Each instance in the dataset is a target word and positive and negative examples of contexts words. Given the target word as input, we'll want to predict (or not predict) these positive and negative context words as outputs using our network. Your task here is to create a python `list` of instances. \n",
    "\n",
    "Your final training data should be a list of tuples in the format ([target_word_id], [word_id_1, ...], [predicted_labels]), where each item in the list is a list:\n",
    "1. The first item is a list consisting only of the target word's ID.\n",
    "2. The second item is a list of word ids for both context words and negative samples \n",
    "3. The third item is a list of labels to predicted for each of the word ids in the second list (i.e., `1` for context words and `0` for negative samples). \n",
    "\n",
    "You will feed these tuples into the PyTorch `DatasetLoader` later that will do the converstion to `Tensor` objects. You will need to make sure that all of the lists in each tuple are `np.array` instances and are not plain python lists for this `Tensor` converstion to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qiam_JFpwtuQ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5831abcb547545f3b06d28afa615ab2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "num_negative_samples_per_target = 2\n",
    "training_data = []\n",
    "# Loop through each token in the corpus and generate an instance for each, \n",
    "# adding it to training_data\n",
    "for i, seq in enumerate(tqdm(corpus.full_token_sequence_as_ids)):\n",
    "    l = len(seq)\n",
    "    n = window_size*2*(1+num_negative_samples_per_target)\n",
    "    if i%10000==9999:\n",
    "        print(i+1)\n",
    "    for j, target in enumerate(seq):\n",
    "        ## avoid creating a training instance when the target word is <UNK>\n",
    "        if corpus.index_to_word[target]=='<UNK>': continue\n",
    "        target_id = [target] # a list of ints(target word ids)\n",
    "        word_id = [] # a list of list(text & negative ids)\n",
    "        label = [] # a list of list(1 for context words and 0 for negative samples).\n",
    "        context = seq[max(0,j-2):j] + seq[(j+1):min(l,j+3)] # a list\n",
    "        if len(context)==0: continue\n",
    "        neg_case = list(corpus.generate_negative_samples(context, len(context)*num_negative_samples_per_target))\n",
    "        word_id = word_id + context + neg_case # 1+2; update word_id\n",
    "        label = label + [1]*len(context) + [0]*len(neg_case) # 1+2; update label         \n",
    "        # n_sup: num of neg cases should be added\n",
    "        # add neg cases to get n \n",
    "        n_sup = n - len(word_id)#n - len(context)*(num_negative_samples_per_target+1)\n",
    "        neg_sup = list(corpus.generate_negative_samples(context, n_sup))\n",
    "        word_id = word_id +  neg_sup\n",
    "        label = label + [0]*n_sup      \n",
    "        training_data.append(tuple([target_id, word_id, label]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMEVVVkDwtuQ"
   },
   "source": [
    "## Create the network\n",
    "\n",
    "We'll create a new neural network as a subclass of `nn.Module` like we did in Homework 1. However, _unlike_ the network you built in Homework 1, we do not need to used linear layers to implement word2vec. Instead, we will use PyTorch's `Emedding` class, which maps an index (e.g., a word id in this case) to an embedding. \n",
    "\n",
    "Roughly speaking, word2vec's network makes a prediction by computing the dot product of the target word's embedding and a context word's embedding and then passing this dot product through the sigmoid function ($\\sigma$) to predict the probability that the context word was actually in the context. The homework write-up has lots of details on how this works. Your `forward()` function will have to implement this computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YrOQmem-wtuQ"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        # Save what state you want and create the embeddings for your\n",
    "        # target and context words\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        # Once created, let's fill the embeddings with non-zero random\n",
    "        # numbers. We need to do this to get the training started. \n",
    "        #\n",
    "        # NOTE: Why do this? Think about what happens if all the embeddings\n",
    "        # are all zeros initially. What would the predictions look like for\n",
    "        # word2vec with these embeddings and how would the updated work?\n",
    "        self.init_emb(init_range=0.5/vocab_size)\n",
    "        pass      \n",
    "    def init_emb(self, init_range):\n",
    "        torch.manual_seed(630)\n",
    "        self.target_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.context_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        # Fill your two embeddings with random numbers uniformly sampled\n",
    "        # between +/- init_range\n",
    "        pass\n",
    "        \n",
    "    def forward(self, target_word_id, context_word_ids):\n",
    "        ''' \n",
    "        Predicts whether each context word was actually in the context of the target word.\n",
    "        The input is a tensor with a single target word's id and a tensor containing each\n",
    "        of the context words' ids (this includes both positive and negative examples).\n",
    "        '''\n",
    "        tensor_target = self.target_embeddings(torch.LongTensor(target_word_id))# tensor:  12 * batch_size* embedding_size\n",
    "        tensor_conte = self.context_embeddings(torch.LongTensor(context_word_ids))\n",
    "        output = torch.sum(torch.mul(tensor_target, tensor_conte),2)\n",
    "        output = torch.sigmoid(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-oWF83ewtuR"
   },
   "source": [
    "## Train the network!\n",
    "\n",
    "Now that you have data in the right format and a neural network designed, it's time to train the network and see if it's all working. The trainin code will look surprisingly similar at times to your pytorch code from Homework 1 since all networks share the same base training setup. However, we'll add a few new elements to get you familiar with more common training techniques. \n",
    "\n",
    "For all steps, be sure to use the hyperparameters values described in the write-up.\n",
    "\n",
    "1. Initialize your optimizer and loss function \n",
    "2. Create your network\n",
    "3. Load your dataset into PyTorch's `DataLoader` class, which will take care of batching and shuffling for us (yay!)\n",
    "4. Create a new `SummaryWriter` to periodically write our running-sum of the loss to a tensorboard\n",
    "5. Train your model \n",
    "\n",
    "Two new elements show up. First, we'll be using `DataLoader` which is going to sample data for us and put it in a batch (and also convert the data to `Tensor` objects. You can iterate over the batches and each iteration will return all the items eventually, one batch at a time (a full epoch's worth).\n",
    "\n",
    "The second new part is using `tensorboard`. As you might have noticed in Homework 1, training neural models can take some time. [TensorBoard](https://www.tensorflow.org/tensorboard/) is a handy web-based view that you can check during training to see how the model is doing. We'll use it here and periodically log a running sum of the loss after a set number of steps. The Homework write up has a plot of what this looks like. We'll be doing something simple here with tensorboard but it will come in handy later as you train larger models (for longer) and may want to visually check if your model is converging. TensorBoard was initially written for another deep learning framework, TensorFlow, but proved so useful it was ported to work in PyTorch too and is [easy to integrate](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html).\n",
    "\n",
    "To start training, we recommend training on the `wiki-bios.10k.txt` dataset. This data is small enough you can get through an epoch in a few minutes (or less) while still being large enough you can test whether the model is learning anything by examining common words. Below this cell we've added a few helper functions that you can use to debug and query your model. In particular, the `get_neighbors()` function is a great way to test: if your model has learned anything, the nearest neighbors for common words should seem reasonable (without having to jump through mental hoops). An easy word to test on the `10k` data is \"january\" which should return month-related words as being most similar.\n",
    "\n",
    "**NOTE**: Since we're training biographies, the text itself will be skewed towards words likely to show up biographices--which isn't necessary like \"regular\" text. You may find that your model has few instances of words you think are common, or that the model learns poor or unusual neighbors for these. When querying the neighbors, it can help to think of which words you think are likely to show up in biographies on Wikipedia and use those as probes to see what the model has learned.\n",
    "\n",
    "Once you're convinced the model is learning, switch to the `med` data and train your model as specified in the PDF. Once trained, save your model using the `save()` function at the end of the notebook. This function records your data in a common format for word2vec vectors and lets you load the vectors into other libraries that have more advanced functionality. In particular, you can use the [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) code in other notebook included to explore the vectors and do simple vector analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Z6XcFW-NwtuR"
   },
   "outputs": [],
   "source": [
    "# TODO: Set your training stuff, hyperparameters, models, tensorboard writer etc. here\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "# del model\n",
    "def training_emb(training_data = training_data,\n",
    "                 batch_size = 16, v = len(corpus.index_to_word), # voc_size\n",
    "                 k = 50, #(embedding size)\n",
    "                 lr = 0.00005, #(learning rate)\n",
    "                 min_token_freq = 5,\n",
    "                 epochs = 1,\n",
    "                 max_step = 10000):\n",
    "    model = Word2Vec(v, k)\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    writer = SummaryWriter()\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "        # TODO: use your DataLoader to iterate over the data\n",
    "        with tqdm(dataloader) as t:\n",
    "            # control the max_step based on the size of batches; 1/1000\n",
    "            # max_step = min(max_step, int(2*t.total/100))\n",
    "            for step, data in enumerate(t):\n",
    "                # TODO: Fill in all the training details here\n",
    "                # NOTE: since you created the data as a tuple of three np.array instances,\n",
    "                # these have now been converted to Tensor objects for us\n",
    "                target_ids, context_ids, labels = data    \n",
    "                # training\n",
    "                # convert to tensors\n",
    "                target_ids = torch.stack(target_ids)\n",
    "                context_ids = torch.stack(context_ids)\n",
    "                labels = torch.stack(labels).type(torch.FloatTensor)\n",
    "                # train the model\n",
    "                outputs = model(target_ids, context_ids)\n",
    "                loss = loss_function(outputs, labels)# Compute the loss. \n",
    "                # zero out the gradients from the old instance\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()# Calculate the gradient and how to update\n",
    "                optimizer.step()# Update the model\n",
    "                loss_sum += loss.item()\n",
    "                if step%100==99:\n",
    "                    writer.add_scalar(\"Loss/train\", loss_sum, step)\n",
    "                    writer.flush()\n",
    "                    loss_sum = 0\n",
    "                    t.update()\n",
    "\n",
    "                if step > max_step:\n",
    "                    break\n",
    "    writer.close()\n",
    "    return model, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68848372e1a49d7aeca264bf78c15cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8920779 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114.633]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c944e259aa94433bb6f115e45191d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2230195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114.633, 30.233]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf84d2e9e0c144a4be97c7ee828a9bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/557549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114.633, 30.233, 19.117]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8552914dec1347ef9350c1a9e647fb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278775 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114.633, 30.233, 19.117, 11.167]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d0fd56fd3447bdb70ce5c160b4310a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114.633, 30.233, 19.117, 11.167, 2.267]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6b82b274144212b419d6ad2c70d5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114.633, 30.233, 19.117, 11.167, 2.267, 1.033]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa6f18f12804ed1ad70fc6bfa26399f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34847 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114.633, 30.233, 19.117, 11.167, 2.267, 1.033, 31.55]\n"
     ]
    }
   ],
   "source": [
    "######## try different batch size\n",
    "all_batch_size = [2, 8, 32, 64, 128, 256, 512]\n",
    "#est_time = []\n",
    "for bs in all_batch_size[-1:]:\n",
    "    model1,t = training_emb(batch_size = bs, max_step = 5000)\n",
    "    time = re.split(r':',re.split(r'[<,]',str(t))[1])\n",
    "    time = sum([int(ele_time)/60**t_unit for t_unit, ele_time in enumerate([0]*(3-len(time))+time)])\n",
    "    est_time.append(round(time,3))\n",
    "    #print(est_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Batch size V.S. Estimated training time(1 epoch)'}, xlabel='Batch size', ylabel='Estimated training time(h/1 epoch)'>"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a plot\n",
    "pd.DataFrame({'est_time':est_time, 'all_batch_size':all_batch_size}).plot(x = 'all_batch_size', y= 'est_time',\n",
    "    xlabel = 'Batch size', ylabel = 'Estimated training time(h/1 epoch)',\n",
    "                        title = 'Batch size V.S. Estimated training time(1 epoch)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Problem 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, I tried to run the training with different batch_size for a few times and get the plot above. \n",
    "As batch_size increases, it appears that estimated running time decreases first, and then began to increase. By checking the memory usage of my computer, I think one possible reason why that the training time increases dramatically when batch_size is 512 is that the memory usage is too high to handle. Given this factor, I think batch_size =256 or 128 is the best batch size that would maximize speed while ensuring sufficient memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94bf774fd6745c0b65fb3dc322df761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model, final_model_timebar = training_emb(batch_size = 256, max_step = 99999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained the model for 1 epoch, and here is a picture of the tensorboard plot from my training run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"tb_pic1.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "345XyXkTwtuS"
   },
   "source": [
    "## Verify things are working\n",
    "\n",
    "Once you have an initial model trained, try using the following code to query the model for what are the nearest neighbor of a word. This code is intended to help you debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7P67GVHjwtuS"
   },
   "outputs": [],
   "source": [
    "def get_neighbors(model, word_to_index, target_word):\n",
    "    \"\"\" \n",
    "    Finds the top 10 most similar words to a target word\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for word, index in tqdm(word_to_index.items(), total=len(word_to_index)):\n",
    "        similarity = compute_cosine_similarity(model, word_to_index, target_word, word)\n",
    "        result = {\"word\": word, \"score\": similarity}\n",
    "        outputs.append(result)\n",
    "\n",
    "    # Sort by highest scores\n",
    "    neighbors = sorted(outputs, key=lambda o: o['score'], reverse=True)\n",
    "    return neighbors[1:11]\n",
    "\n",
    "def compute_cosine_similarity(model, word_to_index, word_one, word_two):\n",
    "    '''\n",
    "    Computes the cosine similarity between the two words\n",
    "    '''\n",
    "    try:\n",
    "        word_one_index = word_to_index[word_one]\n",
    "        word_two_index = word_to_index[word_two]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "    embedding_one = model.target_embeddings(torch.LongTensor([word_one_index]))\n",
    "    embedding_two = model.target_embeddings(torch.LongTensor([word_two_index]))\n",
    "    similarity = 1 - abs(float(cosine(embedding_one.detach().numpy(),\n",
    "                                      embedding_two.detach().numpy())))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "t1GxKMhGwtuS"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e90b93221c4c0f9645c48143c26aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'march', 'score': 0.9999642968177795},\n",
       " {'word': 'june', 'score': 0.9999534487724304},\n",
       " {'word': 'december', 'score': 0.9999221563339233},\n",
       " {'word': 'april', 'score': 0.9999035000801086},\n",
       " {'word': 'october', 'score': 0.9998877048492432},\n",
       " {'word': 'july', 'score': 0.9998447299003601},\n",
       " {'word': 'february', 'score': 0.9998365640640259},\n",
       " {'word': 'september', 'score': 0.9997960329055786},\n",
       " {'word': 'august', 'score': 0.9997805953025818},\n",
       " {'word': 'november', 'score': 0.9996955394744873}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbors(final_model, corpus.word_to_index, \"january\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhmxC8yjwtuS"
   },
   "source": [
    "# Task 2: Save your model!\n",
    "\n",
    "Once you have a fully trained model, save it using the code below. Note that we only save the `target_embeddings` from the model, but you could modify the code if you want to save the context vectors--or even try doing fancier things like saving the concatenation of the two or the average of the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xoR7fJ_awtuT"
   },
   "outputs": [],
   "source": [
    "def save_target(model, corpus, filename):\n",
    "    '''\n",
    "    Saves the model to the specified filename as a gensim KeyedVectors in the\n",
    "    text format so you can load it separately.\n",
    "    '''\n",
    "\n",
    "    # Creates an empty KeyedVectors with our embedding size\n",
    "    kv = KeyedVectors(vector_size=model.embedding_size)        \n",
    "    vectors = []\n",
    "    words = []\n",
    "    # Get the list of words/vectors in a consistent order\n",
    "    for index in trange(model.target_embeddings.num_embeddings):\n",
    "        word = corpus.index_to_word[index]\n",
    "        vectors.append(model.target_embeddings(torch.LongTensor([index])).detach().numpy()[0])\n",
    "        words.append(word)\n",
    "    # Fills the KV object with our data in the right order\n",
    "    kv.add_vectors(words, vectors) \n",
    "    kv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Save Your Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59532ec5e6f8470d8dfacc01bd0a2c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save(final_model, corpus, filename)\n",
    "save_target(final_model, corpus, 'final_model_target_emb.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Qualitative Evaluation of Word Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 14.\n",
    "Load the model (vectors) you saved in Task 2 by using the Jupyter notebook pro\u0002vided (or code that does something similar) that uses the Gensim package to read the vectors.\n",
    "Gensim has a number of useful utilities for working with pretrained vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('final_model_target_emb.kv', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 15. \n",
    "Pick 10 target words and compute the most similar for each using Gensim’s func\u0002tion. Qualitatively looking at the most similar words for each target word, do these predicted word\n",
    "seem to be semantically similar to the target word? Describe what you see in 2-3 sentences. Hint:\n",
    "For maximum effect, try picking words across a range of frequencies (common, occasional, rare\n",
    "words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(corpus.word_counts.keys())[:10]\n",
    "def get_10_tw(corpus=corpus):\n",
    "    corpus_lst = np.array(list(corpus.word_counts.keys()))\n",
    "    corpus_freq = np.array(list(corpus.word_counts.values()))\n",
    "    np.random.seed(666)\n",
    "    com = np.random.choice(corpus_lst[corpus_freq>np.quantile(corpus_freq, 0.99)],3)\n",
    "    mod = np.random.choice(corpus_lst[corpus_freq>np.quantile(corpus_freq, 0.95)],3)\n",
    "    rate = np.random.choice(corpus_lst[corpus_freq<100],4)\n",
    "    return np.concatenate([com,mod,rate],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jersey': 'zealand',\n",
       " 'schools': 'architecture',\n",
       " 'great': 'include',\n",
       " 'silver': 'olympic',\n",
       " 'melbourne': 'sydney',\n",
       " 'brother': 'mother',\n",
       " 'por5': 'militar',\n",
       " 'scalia': 'armaments',\n",
       " 'nascent': 'abolition',\n",
       " 'stp1': '1188'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick 10 target words\n",
    "target_word_10 = get_10_tw(corpus)\n",
    "most_similar_word = []\n",
    "for tw in target_word_10:\n",
    "    sw = word_vectors.similar_by_word(tw)[0][0]\n",
    "    most_similar_word.append(sw)\n",
    "dict(zip(target_word_10,most_similar_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the common words with relatively high frequency(Top 5%), these predicted word usually seem to be semantically similar to the target word. e.g. {'silver': 'olympic','game': 'match', 'who': 'what','Biography': 'Life'}\n",
    "\n",
    "However, for the rare words with relatively lower frequency(usually names), the predicted word seem to be much less similar to the target word. This could be resulted from the lack of training samples, thus the embedding layer are less likely to get trained, which leads to the underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 16. \n",
    "Given the analogy function, find five interesting word analogies with your word2vec\n",
    "model. For example, when representing each word by word vectors, we can generate the following\n",
    "equation, king - man + woman = queen. In other word, you can understand the equation as queen - woman = king - man, which mean the vectors similarity between queen and women is equal to\n",
    "king and man. What kinds of other analogies can you find? (NOTE: Any analogies shown in the\n",
    "class recording cannot be used for this problem.) What approaches worked and what approaches\n",
    "didn’t? Write 2-3 seconds in a cell in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'music'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_analogy(a, b, c):\n",
    "    return word_vectors.most_similar(positive=[b, c], negative=[a])[0][0]\n",
    "get_analogy('man', 'woman', 'king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep\n",
    "get_analogy('child', 'women', 'work')\n",
    "get_analogy('he', 'successful', 'she')\n",
    "get_analogy('win', 'game', 'champion')\n",
    "get_analogy('snow', 'boston', 'sunshine')\n",
    "# not so reasonble\n",
    "get_analogy('man', 'woman', 'king')\n",
    "get_analogy('snow', 'michigan', 'california')\n",
    "get_analogy('men', 'brother', 'her')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the get_analogy() function and model, there are a few equations established, but not all equations seem to be semantically reasonable.\n",
    "\n",
    "For example, 'successful' + 'she' = 'great' + 'he', 'win' + 'third'='game' +'champion' are quite semantically reasonable according to our prior knowledge. I believe the reason why is that these words in the equations are common words that appear frequently, thus the embeddings of these word are well-trained , which makes the predictions reasonable.\n",
    "\n",
    "However, equations like 'michigan'+'california'-'snow'='University' seems much less interpretable. I think this is the case when the equation contains a component word that is highly correlated with a specific word, thus the prediction would be dominated by that word and ignoring other words.\n",
    "\n",
    "Another counter case type that does not work well is the equation like 'brother' + 'her'- 'men'='henriksson'. Sometimes the predictions are rare words like people names or city names. I think this could be resulted from the lack of training on these rare words. I think one possible solution is to train the model for more epoches and get the embeddings of these words better tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Task 4: Debiasing word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PART: DO THIS LAST AND READ CAREFULLY\n",
    "\n",
    "Before you start this part, you need to have a fully working solution and completed the exploratory part of the assignment.\n",
    "\n",
    "**Once you are ready, create a copy of your working notebook and call it `Debiased Word2Vec.ipynb`. Do not do this part in your working code for the assignment!!!**\n",
    "\n",
    "## Seriously, save your code in a new file and then start reading the rest of these instructions there.\n",
    "\n",
    "Ok, hopefully you're reading these in a new file... For this last part of the assignment, we're going to _change_ how word2vec learns at a fundamental level. \n",
    "\n",
    "As you might have noticed in your exploratory analysis, the word2vec model learns to weird and sometimes biased associations between words. In particular, your word2vec model has likely learned some unfortunate gender biases, e.g., that the vector for \"nurse\" is closer to \"woman\" than \"man\". The algorithm itself isn't to blame since it is learning these from a corpus (here, Wikipedia biographies) that contain biases already based on how people write. Wikipedia [is](http://markusstrohmaier.info/documents/2015_icwsm2015_wikipedia_gender.pdf) [well](http://dcs.gla.ac.uk/~mounia/Papers/wiki_gender_bias.pdf) [known](https://www.academia.edu/download/64856696/genderanalysisofWikipediabiostext_self_archived.pdf) for having gender biases in how it writes about men and women.\n",
    "\n",
    "\n",
    "**Side note**: Some times this bias-learning behavior is useful: We can use word2vec to uncover these biases and analyze their trends, like this PNAS paper did for [looking at bias in news writing along multiple dimensions of identity](https://www.pnas.org/content/pnas/115/16/E3635.full.pdf)\n",
    "\n",
    "In this last part of the homework, we'll ask how we might try to _prevent_ these biases by modifying the training. You won't need to solve this problem by any means, but the act of trying to reduce the biases will open up a whole new toolbox for how you (the experimenter/practioner) can change how and what models learn.\n",
    "\n",
    "There are many potential ways to _debias_ word embeddings so that their representations are not skewed along one \"latent dimension\" like gender. In this homework, you'll be trying one of a few different ideas for how to do it. **You are not expected to solve gender bias! This part of the assignment is to have to start grappling with a hard challenge but there is no penalty for doing less-well!** \n",
    "\n",
    "One common technique to have models avoid learning bias is similar to another one you already&mdash;**regularization**. In Logistic Regression, we could use L2 regularization to have our model avoid learning $\\beta$ weights that are overfit to specific or low-frequency features by adding a regularizer penalty where the larger the weight, the more penalty the model paid. Recall that this forces the model to only pick the most useful (generalizable) weights, since it has to pay a penalty for any non-zero weight. \n",
    "\n",
    "In word2vec, we can adapt the idea to think about whether our model's embeddings are closer or farther to different gender dimensions. For example, if we consider the embedding for \"president\", ideally, we'd want it to be equally similar to the embeddings for \"man\" and \"woman\". One idea then is to penalize the model based on how uneven the similarity is. We can do this by directly modifying the loss:\n",
    "```\n",
    "loss = loss_criteron(preds, actual_vals) + some_bias_measuring_function(model)\n",
    "```\n",
    "Here, the `some_bias_measuring_function` function takes in your model as input and returns how much bias you found. Continuing our example, we might implement it in pseudocode as\n",
    "```\n",
    "def some_bias_measuring_function(model):\n",
    "    pres_woman_sim = cosine_similarity(model, \"president\", \"woman\")\n",
    "    pres_man_sim = cosine_similarity(model, \"president\", \"man\")\n",
    "    return abs(pres_woman_sim - pres_man_sim)\n",
    "```\n",
    "This simple example would penalize the model for learning a representation of \"president\" that is more simular to one of the two words. Of course, this example is overly simple. Why just \"president\"? Why just \"man\" and \"woman\"? Why not other words or other gender-related words or other gender identities?? \n",
    "\n",
    "Another idea might be to just make the vectors for \"man\" and \"woman\" be as similar as possible:\n",
    "```\n",
    "def some_bias_measuring_function(model):\n",
    "    # cosine similarity is in [-1,1] but we mostly expect it in [0,1]\n",
    "    man_woman_sim = cosine_similarity(model, \"man\", \"woman\")\n",
    "    # penalize vectors that are not maximally similar, and avoid the edge case \n",
    "    # of negative cosine similarity\n",
    "    return 1 - max(man_woman_sim, 0)\n",
    "```\n",
    "\n",
    "All of this works in practice because PyTorch is fantastic about tracking the gradient with respect to the loss. This ability lets us easily define a loss function so that our word2vec model (1) learns to predict the right context words while (2) avoids learning biases. If we compare this code to the numpy part of Homework 1, it's easy to see how powerful PyTorch can be as an experimenter for helping you control what and how your models learn!\n",
    "\n",
    "Your task is to expand this general approach by coming up with an extension to word2vec that adds some new term to the `loss` value that penalizes bias in the gender dimension. There is no right way to do this and even some right-looking approaches may not work&mdash;or might word but simultaneously destroy the information in the word vectors (all-zero vectors are unbiased but also uninformative!). \n",
    "\n",
    "**Suggestion:** You may need to weight your bias term in the loss function (remember that $\\lambda_1 x_1 + \\lambda_2 x_2$ interpolation? This is sort of similar) so that your debiasing regularizer doesn't overly penalize your model.\n",
    "\n",
    "Once you have generated your model, record word vector similarities for the pairs listed on canvas in `word-pair-similarity-predictions.csv` where your file writes a result like\n",
    "```\n",
    "word1,word2,sim\n",
    "dog,puppy,0.91234123\n",
    "woman,preseident,0.81234\n",
    "```\n",
    "You'll record the similarity for each pair of words in the file and upload it to CodaLab, which is kind of like Kaggle but lets use a custom scoring program. We'll evaluate your embeddings based on how unbiased they are and how much information they still capture after debiasing. **Your grade does not depend on how well you do in CodaLab, just that you tried something and submitted.** However, the CodaLab leaderboard will hopefully provide a fun and insightful way of comparing just how much bias we can remove from our embeddings.\n",
    "\n",
    "The CodaLab link will be posted to Piazza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(model, ind1, ind2, corpus=corpus):\n",
    "#     ind1 = corpus.word_to_index[w1]\n",
    "#     ind2 = corpus.word_to_index[w2]\n",
    "    return F.cosine_similarity(model.target_embeddings(ind1), \n",
    "                               model.target_embeddings(torch.LongTensor([ind2])),\n",
    "                               dim=2\n",
    "                              )\n",
    "# model; corpus; target_id tensor;\n",
    "def gender_bias_measuring_fun(model, corpus, target, gender1=\"woman\", gender2=\"man\"):\n",
    "    g_id1 = corpus.word_to_index[\"woman\"]\n",
    "    g_id2 = corpus.word_to_index[\"man\"]\n",
    "    pres_woman_sim = cosine_similarity(model, target, g_id1)\n",
    "    pres_man_sim = cosine_similarity(model, target, g_id2)\n",
    "    bias_loss = torch.abs(torch.subtract(pres_woman_sim,pres_man_sim))\n",
    "    return torch.mean(bias_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_emb_debiase(training_data = training_data,\n",
    "                 batch_size = 16,v = len(corpus.index_to_word), # voc_size\n",
    "                 k = 50, #(embedding size)\n",
    "                 lr = 0.00005, #(learning rate)\n",
    "                 min_token_freq = 5,\n",
    "                 epochs = 1,\n",
    "                 max_step = 10000,\n",
    "                lam = 0.1):\n",
    "    model = Word2Vec(v, k)\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    writer = SummaryWriter()\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "        # TODO: use your DataLoader to iterate over the data\n",
    "        with tqdm(dataloader) as t:\n",
    "            # control the max_step based on the size of batches; 1/1000\n",
    "            # max_step = min(max_step, int(2*t.total/100))\n",
    "            for step, data in enumerate(t):\n",
    "                target_ids, context_ids, labels = data    \n",
    "                # training convert to tensors\n",
    "                target_ids = torch.stack(target_ids)\n",
    "                context_ids = torch.stack(context_ids)\n",
    "                labels = torch.stack(labels).type(torch.FloatTensor)\n",
    "                # train the model\n",
    "                outputs = model(target_ids, context_ids)\n",
    "                ####### correct each target wrods bias on woman/man\n",
    "                loss = loss_function(outputs, labels) + lam*gender_bias_measuring_fun(model, corpus, target_ids)\n",
    "                # Compute the loss. \n",
    "                # zero out the gradients from the old instance\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()# Calculate the gradient and how to update\n",
    "                optimizer.step()# Update the model\n",
    "                loss_sum += loss.item()\n",
    "                if step%100==99:\n",
    "                    writer.add_scalar(\"Loss/train\", loss_sum, step)\n",
    "                    writer.flush()\n",
    "                    loss_sum = 0\n",
    "                    t.update()\n",
    "                if step > max_step:\n",
    "                    break\n",
    "\n",
    "    # once you finish training, it's good practice to switch to eval.\n",
    "    writer.close()\n",
    "    return model, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa9099927724fba98bf3b940190930c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deb_model, deb_model_timebar = training_emb_debiase(batch_size = 256, max_step = 99000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ddf018867f40119326e3199a28c36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_target(deb_model, corpus, 'deb_model_target_emb.kv')\n",
    "deb_word_vectors = word_vectors = KeyedVectors.load_word2vec_format('deb_model_target_emb.kv', binary=False)\n",
    "similar_data = pd.read_csv('./word_pair_similarity_predictions.csv')\n",
    "pred_similar = []\n",
    "# give corpus; model\n",
    "for j in range(similar_data.shape[0]):\n",
    "    ind1 = corpus.word_to_index[similar_data.iloc[j,0]]\n",
    "    ind2 = corpus.word_to_index[similar_data.iloc[j,1]]\n",
    "    pred_similar.append(deb_word_vectors.n_similarity(similar_data.iloc[j,0], similar_data.iloc[j,1]))\n",
    "similar_data['sim'] =pred_similar\n",
    "similar_data.head()\n",
    "similar_data.to_csv('output_word_pair_similarity_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMX00MxKwtuT"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_Word2Vec_Wkailin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
